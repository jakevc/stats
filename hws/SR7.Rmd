---
title: "SR7"
author: "Jake VanCampen"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
fig.dim <- 3
knitr::opts_chunk$set(fig.width=2*fig.dim,
                      fig.height=fig.dim,
                      fig.align='center',
                      message=FALSE)
```


## Robust regressison 

The data used here are from the 'lars' packaage. They consist of 442 rows, each representing a diabetes patient, and 10 columns representing the patients Age, Sex, Body Mass Index, Average Blood Pressure, and six blood serum measurements. The response 'y' represents a continuous measure of disease progression one year after baseline. 

Pairwise interactions of all variables show no significatnt outliers. Importantly, the response variable is not normally distributed. This can be seen by plotting the response vs. the residuals of the Ordinary Least Squares (OLS) regression. The residuals seem to increase with the response variable. Because there are 10 predictor variables and all their interactions, it is likely that OLS is overfitting the data resulting in a noisy fit. 

We can use robust regression to more effectively model the extra noisy reponse variable. The idea here is to use a sparsifying prior on the coefficients of the predictors ($\beta$) such that they are highly encouraged to be zero, this draws out only the most important predictors, so that the importance coefficients come out of the 64 terms. 

$$ y = \beta_0 + \beta_1 * x_1 ... \beta_n * x_n + \epsilon $$

The sparsifying prior used here is an alternate parameterization of the 'horseshoe prior', which is a scale-mixture of normals resulting in both a computationally efficient, and strongly sparsifying prior. The following realtionship describes this sparse prior on $\beta$ used in out example.

$$ d_{beta}, d_a \sim Normal(0,1) $$

$$ d_b \sim Gamma((0.5, 0.5)* \nu) $$
$$ \nu \sim Normal(0,20) $$

$$\tau \sim Uniform(0,1)$$
$$ \beta \sim  d_{beta} *  d_a * \sqrt{d_b} * \tau $$

The data were fit using OLS, and using a model with the above parameterization of the horseshoe prior on the coefficients $\beta$. Five fold cross validation was used to compare the cross validation error of each fit. 

The following data show root-mean-square (RMS) prediction error for each 'horseshoe' and 'ols' models for each cross validation. 

```
                [,1]      [,2]      [,3]      [,4]      [,5]
ols_pred       0.6082998 0.8078309 0.6883125 0.6305012 0.7113227
horseshoe_pred 0.5936128 0.6519209 0.6345916 0.5992874 0.6417806
ols_fit        0.5572134 0.5394652 0.5440365 0.5492974 0.5367800
horseshoe_fit  0.5997650 0.5801887 0.5945189 0.5963294 0.5803999
```

The mean RMS prediction error across cross validations can be calculated as a row mean for each method: 

```
ols_pred_mean         0.6892534 
horseshoe_pred_mean   0.6242387
```

The horseshoe method shows a `r 0.6892534 - 0.6242387` lower mean RMS prediction error for five-fold cross validatiaon.

A plot of the coefficients of the regression for each method is shown below. Along with their 95% confidence interval. This plot clearly shows the effect of the strongly sparsifying prior, where in the sparse regression more of the coefficients are around zero, with those that drive the response more clearly distinguishable as signal. The OLS regression is by comparison more noisy and likely overfitting some interaction terms (coefficients between 45 and 50). 

![Sparse_Coefficients](sprs.png)

Next, the coefficients of the horseshoe fit with a 95% CI not overlapping zero were determined. Those coefficients represented the following predictors: sex, bp, bmi, and ltg. 


Sex, bp, bmi, and ltg were then used to predict the response 'y', and the prediction was plotted vs. the observed value for both methods. The root-mean-square error for each prediction is shown above each plot. The horseshoe predictions shows a much lower RMSE prediction value than that of the OLS. 

![](compare_predict.png)

## Final thoughts

These results display the usefullness of the horseshoe prior for modeling data that are easily subject to overfitting using methods that assume normally distributed data. It is shown that sparsificaiton of the model coefficients using this method increase the signal to noise of the estimate in the coefficients, and that using the horseshoe model to fit the data results in a more accurate prediction of the response variable, given the 'important' coefficients.





