{
    "collab_server" : "",
    "contents" : "---\ntitle: \"Notes\"\nauthor: \"Jake VanCampen\"\ndate: \"10/10/2017\"\noutput: pdf_document\n---\n\n```{r}\ntitle <- rmarkdown::metadata$title\n\n```\n\n\n```{r}\nknitr::opts_chunk$set(fig.path='{{ site.url }}/images/exploring-the-cars-dataset-')\n```\n## Week 3\n\n## Oct 10, 2017\n\n## CODE\n```{r}\nlibrary(tidyverse)\n```\n\nWorking with the diamonds dataset: categorical variables can be visualized using a geom_bar: \n\n```{r}\nggplot(data = diamonds)+ \n  geom_bar(aes(x = cut, fill = cut))+\n  xlab('')+ \n  theme_classic()\n\n```\n\nThe geom_histogram and geom_freqpoly are more useful for binning continuous variables: \n\n```{r}\nggplot(data = diamonds) +\n  geom_histogram(aes(x = carat, fill = cut, binwidth = 0.5))\n```\n\n\nThis histogram is cool, but it's harder to tell which cut is more prevalent, freqpoly is helpful for this reason. We can also add facets, and a nice title: \n\n```{r}\nggplot(data = diamonds) +\n   geom_freqpoly(aes(x = carat, color = cut), bins = 40)+\n   facet_wrap(~clarity)+ \n   ggtitle('Frequency histograms of cut for all clarity specifications')\n```\n\n\nA ggplot example using the mpg dataset:\n```{r}\nlibrary(tidyverse)\n\nggplot(mpg, aes(displ, hwy))+ \n  geom_point()+ \n  geom_smooth(se = FALSE)+\n  labs(title = 'Fuel efficiency generally decreases with engine size')\n\n```\n\n\n## Lecture\n\nR markdown used for reproducible data analysis. R markdown allows for \n\n - better statistical programming skills\n - cleaner analysis\n - transparent reproducibility\n - easier communication \n - richer understanding of your data \n \n## Sampling and estimation \n\nThe goal of replication is to quantify variation at as many levels in study as possible. We want to think about both accuracy and precision while collecting these data. The target represents a parameter that we would like to estimate, accuracy represents how close we are to estimating the parameter. Precision will not get us closer to the parameter. We want estimates that are unbiased, and precise as possible. \n\nThe goal of **randomizaiton** is to avoid bias as much as possible.\n\n\n**Accuracy** is the closeness of an estimated value to it's true value. \n\n\n**Precision** is the closeness of repeted estimates to one another. \n\n\nWe want to have unbiased estimates that are the most precise. \n\n\nParametric (a few special exceptions, like the sample mean and its standard error for a normal distribution)\n\n\n* Ordinary Least Squares (OLS) - optimized procedure that\n* produces one definitive result, easy to use but no estimates of\nconfidence\n* Resampling - bootstrapping and randomization\n* Maximum Likelihood (ML) - Can provide model-based\nestimates with confidence, but harder to calculate\n* Bayesian Approaches - a very different way of looking at the world\n\n\nParameter estimation \n\n* the process of inferring a populaiton parameter from sample data \n\n* the value of a sample estimate is almost never the same as the population parameter because of random sampling error. \n\n* Sampling distribution of an estimate. \n   - All values of the parameter we might have obtained from our sample \n   \n\n## Bootstrapping to Produce a CI\n\nStarting with a vector x, first we make 1000 bootstrap replicates of a size 10 sample with replacement. Then the mean of each of the bootstrap replicate is taken and added to a vector z. \n\n```{r}\nx <- c(0.9, 1.2, 1.2, 1.3, 1.4, 1.4, 1.6, 1.6, 2.0, 2.0)\n\nz <- NULL \nfor (i in 1:1000) {\n  xboot <- sample(x, 10, replace = T)\n  z[i] <- mean(xboot)\n}\n```\n\nLet's plot a histogram of the 100 bootstrap replicates, and calculate the standard deviation of the resampled means: \n \n```{r}\nhist(z)\npaste('Standard Deviation of resampled means', \n      sd(z), sep = ': ')\n```\n\nLet's compare the standard deviation of the resampled means to the standard error of the mean from our original sample, x.\n\n```{r}\npaste('Standard error of the original mean', \n      sd(x)/sqrt(10), sep = ': ') \n```\n\nIt is evident that the standard deviation of the resampled means is slightly less than the standard error of the original mean. \n\n\nNow let's look at bootstrapping some real data. I will go ahead and calculate a bootstrapped estimate of the mean expression level of one gene from the `Gacu_RNAseq_Subset.csv` dataset.\n\n```{r}\n# load packages\nlibrary(tidyverse)\n\n# read in data\nGacu_data <- read.csv('Data/GacuRNAseq_Subset.csv', header = T) %>% as.tibble()\n\n# take the expression levels for one gene \ng3 <- Gacu_data$ENSGACG00000000003\n\n# how are they distributed\nhist(g3) \n\n```\n\nIt's hard to tell form this small sample, but these data may not be normally distributed, making it hard to do statistical transformations on these data. To deal with this we take bootstrap replicates of the data, resampling the 'population' so that it models a gaussian.\n\n\n```{r}\n# initialize empty vector to hold the means\ng3_means <- NULL\n\n# sample from the g3 vector 1000 times and generate means\nfor (i in 1:1000){\n  g3_samp <- sample(g3, 1000, replace = T)\n  g3_means[i] <- mean(g3_samp)\n}\n\n# create a histogram of the resampled means\nhist(g3_means)\n```\n\nThis looks more normally distributed!\n\nLet's add confidence intervals to the graph using the quantile function. \n\n```{r}\ng3_ci <- quantile(g3_means, c(.025, 0.95))\nprint(g3_ci)\n\nhist(g3_means,\n     main = 'Histogram of the bootstrapped means for g3')\nfor (i in 1:length(g3_ci)){\n  abline(v = g3_ci[i], col = 'red')\n}\n```\n\nThe 95% confidence interval is shown with vertical red bars on the distribution of g3 means. This means we are 95% confident that the true mean lies within this interval of the bootstapped mean. \n\n\nNow that we have found the bootstrap distribution of the mean for g3, lets find the Bootstrap distribution for the variance! \n\n```{r}\n# initialize empty vector to hold the means\ng3_var <- NULL\n\n# sample from the g3 vector 1000 times and generate means\nfor (i in 1:1000){\n  g3_samp <- sample(g3, 1000, replace = T)\n  g3_var[i] <- var(g3_samp)\n}\n\n# create a histogram of the resampled means\nhist(g3_var, \n     main = 'Histogram of bootstrapped variance for g3')\n\n```\n\n\n\nHow about a bootstrapped distribution of the standard deviation? Same method really!\n\n```{r}\n# initialize empty vector to hold the means\ng3_sd <- NULL\n\n# sample from the g3 vector 1000 times and generate means\nfor (i in 1:1000){\n  g3_samp <- sample(g3, 1000, replace = T)\n  g3_sd[i] <- sd(g3_samp)\n}\n\n# create a histogram of the resampled means\nhist(g3_sd, \n     main = 'Histogram of bootstrapped standard deviation for g3')\n\n```\n\n\n\n## Thursday, Oct 12\n\n### Parameter Estimation - Ordinary Least Squares (OLS) \n\n* Algorithmic approach to parameter estimatation. \n* Oldest and best developed statistical approach\n* used extensively in linear models (ANOVA and regression) \n* often only gives a single best estimate, but no confidence intervals\n* can use **resampling** approaches to get CIs\n* many OLS estimators have been duplicated in ML estimators\n\n\n### The bootstrap algorithm \n\n* Take a random sample of individuals from the original data \n* calculate the estimate using measurments in the bootstrap sample from step 1. The first **bootstrap replicate sample** \n\n* repeating steps 1 and 2 a large number of times (1000) \n* calculate the sample standard deviation of all the bootstrap replicate estimates obtained in step 3. \n* the resulting quantity is teh bootstrap standard error. \n\n\n\n### Why bootstrap? \n\n* can be applied to almost any sample statistic (mean, proportionm correlation, regression) \n* works when there is not ready formula for standard error\n* nonparametric so doesn't require normally distributed data \n* works for estimates based on complicated sampling procedures or calculations. \n\n\n\n## Parameter Estimation - Maximum Likelihood (ML)\n\n\nWe usually use starting assumptions to fix the probability of a parameter value. \n\n\nWith maximum liklihood, the parameter is the unknwon. ML uses a range of parameter values to determine the probability of obtaining the observed value. \n\n\nThe **probability** of an event is the proportion of times that the event would occur if we repeated a random trial over and over. \n\n\nA **probability distribution** is a list of all mutually exclusive outcomes of a random trial and their probabilities of ccurance. \n\n\nLikelihood is a conditional probability\n\nThe **likelihood** of a population parameter equaling a specific value, given the data, is the probability of obtaining the observed data **given** that the population parameter equals the specific value. \n\n\n\nL[parameter | date] = Pr[ data|parameter ]\n\n\nA **likelihood function** is the range of probabilities for the data over the range of possible parameter values. \n\n\nFor example: \n\n\nData: The Hny wasp, Trichogramma brassicae, rides on female\ncabbage white bu`erflies, Pieris brassicae. When a bu`erfly lays her eggs on a cabbage, the wasp climbs down and parasiHzes the freshly laid eggs. Fatouros et al. (2005) carried out trials to determine whether the wasps can dis2nguish mated female bu7erflies from unmated females. In each trial a single wasp was presented with two female cabbage white bu`erflies, one a virgin female, the other recently mated. Y = 23 of 32 wasps tested chose the mated female. What is the proporHon p of wasps in the populaHon choosing the mated female?\n\n\nWe want to estimate the parameter based on the data! \n This is a binomially distributed situation so, in R, assuming that the probability of choseing is 0.5 \n \n \n```{r}\ndbinom(23, 32, prob = 0.5) \n```\n\n\nLet's work with the log-liklihoods \n\nln L[0.5 | 23 choose mated] ln[(32/23)] + 23ln[0.5] + 9ln[1-0.5]\n\n\nin R:\n\n\n```{r}\ndbinom(23,32,prob = 0.5, log = T)\n```\n\n\n**Likelihood works backward from probability** \nTypically we use probability to predict unkn data outcomes based on known parameters. Here we use likelihood to estimate unkn params based on known data. \n\n\n**The likelihood function is not a probability distribution**\nThe populaHon proporHon p is the variable of the funcHon, but it is not a random variable (its\nvalue is not determined by random trial). The likelihood raHo (difference of log-likelihood)\nmeasures relaHve support for alternaHve parameter values\n\n\n\n**Maximum likelihood estimate**\nThe ML estimate could have been obtained more easily a that. \n\nThe conventional formula for estimating a proportion yields the ML estimate. Most estimates are ML estimates\n\n\n### Write a Maximum Liklihood Function!\n\nCalculate an ML estimate! \n\n```{r}\nx <- seq(0, 1.0, by = 0.01)\ny <- dbinom(65, 100, x) \nplot(x, y, type = 'l')\n```\n\n\nWe usually work with log values of likelihoods: \n\n```{r}\nx <- seq(0, 1.0, by = 0.01)\ny <- dbinom(65, 100, x) \nln_y <- log(y) \nplot(x, ln_y, type = 'l') \n```\n\nPlot the likelihood curve of the allele frequency in a populaiton after examining 10,000 gametes and finding 3,594 of them to contain one allele, and 6,406 containing another allele: \n\n```{r}\nx <- seq(0, 1, by = 0.001)\ny <- dbinom(3594,10000, x)\nln_y <- log(y)\nplot(x, ln_y, type = 'l')\n```\n\n\nNow we can inspect the maximum in a dataframe\n\n```{r}\nln_y_func <- data.frame(x, ln_y)\nhead(ln_y_func)\n```\n\n\nWhere is the ln_y_func maximized? Use subset to get the maximum value: \n\n```{r}\nmax_prob <- subset(ln_y_func, y==max(y))\nmax_prob\n```\n\n\nThe maximum liklihood estimate is returned at 0.359. Let's determine the confidence interval of this estimate. \n\n\n```{r}\nci_y <- subset(ln_y_func, y>=max(y)-1.92) \nhead(ci_y)\n```\n\n\nApproximate the CI by minimizing and maximizing ci_y\n\n```{r}\nmax(ci_y$x)\nmin(ci_y$x) \n```\n\n\n## Bayesian Look at estimation\n\nA way of quantifying uncertainty of the populaiton parameter, not our estimate of it, using prior information about the probability distribution of the parameter. \n\n\n## Hypothesis testing and p-values \n\n* a statement of belief about the world \n* need to critically test \n   - to accept of reject the hypothesis\n   - compare the relative merits of different models \n* this is why we use statistical sampling distributions. \n\n\nHO: Null hypothesis: Ducks and beavers are the same height\n\nH1: Alternative hypothesis: Ducks and beavers are different heights\n\n** What is the probability that we would reject a true null hypothesis? ** \n\n** What is the probability that we would accept a false null hypothesis?** \n\nAssuming a normal distribution: \n\nThe t-statistic will get larger when the means are more different as compared to the standard deviations and the t-statistic will get smaller when the standard deviations are different compared to the difference in the means. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
    "created" : 1508356502777.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "490195608",
    "id" : "FFA4933E",
    "lastKnownWriteTime" : 1508258396,
    "last_content_update" : 1508258396,
    "path" : "~/Documents/Bi610/Oct_09_2017/w3_notes.Rmd",
    "project_path" : null,
    "properties" : {
    },
    "relative_order" : 3,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_markdown"
}